{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T12:43:19.367120Z",
     "iopub.status.busy": "2025-08-24T12:43:19.366570Z",
     "iopub.status.idle": "2025-08-24T12:44:54.271621Z",
     "shell.execute_reply": "2025-08-24T12:44:54.270533Z",
     "shell.execute_reply.started": "2025-08-24T12:43:19.367095Z"
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Fix truncated images\n",
    "# --------------------------\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# --------------------------\n",
    "# Imports\n",
    "# --------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input\n",
    "from sklearn.utils import resample\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# --------------------------\n",
    "# Mixed precision for speed\n",
    "# --------------------------\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# --------------------------\n",
    "# Paths (nested dataset)\n",
    "# --------------------------\n",
    "base_dir = \"/kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification\"\n",
    "\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "val_dir   = os.path.join(base_dir, \"val\")\n",
    "test_dir  = os.path.join(base_dir, \"test\")\n",
    "\n",
    "print(\"Train classes:\", os.listdir(train_dir))\n",
    "print(\"Val classes:\", os.listdir(val_dir))\n",
    "print(\"Test classes:\", os.listdir(test_dir))\n",
    "\n",
    "# --------------------------\n",
    "# Balance training dataset by oversampling\n",
    "# --------------------------\n",
    "def balance_dataset(directory):\n",
    "    classes = os.listdir(directory)\n",
    "    class_files = {cls: [os.path.join(directory, cls, f) for f in os.listdir(os.path.join(directory, cls))] for cls in classes}\n",
    "    max_count = max(len(files) for files in class_files.values())\n",
    "    balanced_files = []\n",
    "    for cls, files in class_files.items():\n",
    "        if len(files) < max_count:\n",
    "            files = resample(files, replace=True, n_samples=max_count, random_state=42)\n",
    "        balanced_files.extend(files)\n",
    "    return balanced_files, classes\n",
    "\n",
    "balanced_train_files, train_classes = balance_dataset(train_dir)\n",
    "print(\"Balanced training files per class:\", {cls: len([f for f in balanced_train_files if cls in f]) for cls in train_classes})\n",
    "\n",
    "# --------------------------\n",
    "# Parameters\n",
    "# --------------------------\n",
    "img_size = (160, 160)  # smaller for faster training\n",
    "batch_size = 32        # larger batch for speed\n",
    "\n",
    "# --------------------------\n",
    "# Data Generators\n",
    "# --------------------------\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8,1.2]\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    train_dir, target_size=img_size, batch_size=batch_size, class_mode='binary'\n",
    ")\n",
    "\n",
    "val_gen = val_test_datagen.flow_from_directory(\n",
    "    val_dir, target_size=img_size, batch_size=batch_size, class_mode='binary'\n",
    ")\n",
    "\n",
    "test_gen = val_test_datagen.flow_from_directory(\n",
    "    test_dir, target_size=img_size, batch_size=batch_size, class_mode='binary', shuffle=False\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# DenseNet121 Model\n",
    "# --------------------------\n",
    "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=img_size+(3,))\n",
    "base_model.trainable = False  # freeze all first\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(1, activation='sigmoid', dtype='float32')  # force float32\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Callbacks\n",
    "# --------------------------\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6, verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='/kaggle/working/fracture_model_best.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# --------------------------\n",
    "# Train Phase 1: Frozen base\n",
    "# --------------------------\n",
    "history1 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=7,\n",
    "    steps_per_epoch=len(train_gen)//2,\n",
    "    validation_steps=len(val_gen)//2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Fine-tune last DenseNet layers\n",
    "# --------------------------\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-50]:  # only fine-tune last 50 layers\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(1e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(train_gen)//2,\n",
    "    validation_steps=len(val_gen)//2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Save final model with timestamp\n",
    "# --------------------------\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "final_model_name = f\"/kaggle/working/fracture_model_final_{timestamp}.keras\"\n",
    "model.save(final_model_name)\n",
    "print(f\"Final model saved as {final_model_name}\")\n",
    "\n",
    "# --------------------------\n",
    "# Evaluate on test set\n",
    "# --------------------------\n",
    "loss, acc = model.evaluate(test_gen)\n",
    "print(f\"\\nFinal Test Accuracy: {acc:.4f}, Test Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T12:45:21.613961Z",
     "iopub.status.busy": "2025-08-24T12:45:21.613030Z",
     "iopub.status.idle": "2025-08-24T12:45:23.810473Z",
     "shell.execute_reply": "2025-08-24T12:45:23.809512Z",
     "shell.execute_reply.started": "2025-08-24T12:45:21.613931Z"
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Fix truncated images\n",
    "# --------------------------\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# --------------------------\n",
    "# Imports\n",
    "# --------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import mixed_precision\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# --------------------------\n",
    "# Mixed precision for speed\n",
    "# --------------------------\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# --------------------------\n",
    "# Paths\n",
    "# --------------------------\n",
    "base_dir = Path(\"/kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification\")\n",
    "\n",
    "train_dir = base_dir / \"train\"\n",
    "val_dir   = base_dir / \"val\"\n",
    "test_dir  = base_dir / \"test\"\n",
    "\n",
    "# --------------------------\n",
    "# Create truly balanced training dataset\n",
    "# --------------------------\n",
    "classes = [\"Fractured\", \"Non-Fractured\"]\n",
    "\n",
    "# Count images\n",
    "counts = {cls: len(list((train_dir/cls).glob(\"*\"))) for cls in classes}\n",
    "min_count = min(counts.values())\n",
    "\n",
    "# Balanced training directory\n",
    "balanced_train_dir = Path(\"/kaggle/working/train_balanced\")\n",
    "if balanced_train_dir.exists():\n",
    "    shutil.rmtree(balanced_train_dir)\n",
    "balanced_train_dir.mkdir()\n",
    "\n",
    "for cls in classes:\n",
    "    (balanced_train_dir/cls).mkdir(parents=True, exist_ok=True)\n",
    "    files = list((train_dir/cls).glob(\"*\"))\n",
    "    sampled_files = np.random.choice(files, min_count, replace=False)\n",
    "    for f in sampled_files:\n",
    "        shutil.copy(f, balanced_train_dir/cls/f.name)\n",
    "\n",
    "print(\"Balanced dataset created:\")\n",
    "for cls in classes:\n",
    "    print(cls, len(list((balanced_train_dir/cls).glob(\"*\"))))\n",
    "\n",
    "# --------------------------\n",
    "# Parameters\n",
    "# --------------------------\n",
    "img_size = (160, 160)\n",
    "batch_size = 32\n",
    "\n",
    "# --------------------------\n",
    "# Data Generators\n",
    "# --------------------------\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8,1.2]\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    balanced_train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = val_test_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_gen = val_test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# DenseNet121 Model\n",
    "# --------------------------\n",
    "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=img_size+(3,))\n",
    "base_model.trainable = False  # freeze all first\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(1, activation='sigmoid', dtype='float32')  # force float32\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Callbacks\n",
    "# --------------------------\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6, verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='/kaggle/working/fracture_model_best.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# --------------------------\n",
    "# Train Phase 1: Frozen base\n",
    "# --------------------------\n",
    "history1 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=7,\n",
    "    steps_per_epoch=len(train_gen)//2,\n",
    "    validation_steps=len(val_gen)//2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Fine-tune last DenseNet layers\n",
    "# --------------------------\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-50]:  # fine-tune last 50 layers\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(1e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(train_gen)//2,\n",
    "    validation_steps=len(val_gen)//2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Save final model with timestamp\n",
    "# --------------------------\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "final_model_name = f\"/kaggle/working/fracture_model_final_{timestamp}.keras\"\n",
    "model.save(final_model_name)\n",
    "print(f\"Final model saved as {final_model_name}\")\n",
    "\n",
    "# --------------------------\n",
    "# Evaluate on test set\n",
    "# --------------------------\n",
    "loss, acc = model.evaluate(test_gen)\n",
    "print(f\"\\nFinal Test Accuracy: {acc:.4f}, Test Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T12:47:59.557752Z",
     "iopub.status.busy": "2025-08-24T12:47:59.557487Z",
     "iopub.status.idle": "2025-08-24T12:47:59.591991Z",
     "shell.execute_reply": "2025-08-24T12:47:59.590990Z",
     "shell.execute_reply.started": "2025-08-24T12:47:59.557733Z"
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Fix truncated images\n",
    "# --------------------------\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# --------------------------\n",
    "# Imports\n",
    "# --------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# --------------------------\n",
    "# Mixed precision\n",
    "# --------------------------\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# --------------------------\n",
    "# Paths\n",
    "# --------------------------\n",
    "train_dir = \"/kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/train\"\n",
    "val_dir   = \"/kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/val\"\n",
    "test_dir  = \"/kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/test\"\n",
    "\n",
    "classes = [\"Fractured\", \"Non-Fractured\"]\n",
    "img_size = (160, 160)\n",
    "batch_size = 32\n",
    "\n",
    "# --------------------------\n",
    "# Prepare true balanced training generator\n",
    "# --------------------------\n",
    "file_paths = {cls: [os.path.join(train_dir, cls, f) for f in os.listdir(os.path.join(train_dir, cls))] for cls in classes}\n",
    "max_count = max(len(file_paths[cls]) for cls in classes)\n",
    "\n",
    "# Oversample minority class\n",
    "for cls in classes:\n",
    "    n = len(file_paths[cls])\n",
    "    if n < max_count:\n",
    "        extra = np.random.choice(file_paths[cls], max_count - n, replace=True)\n",
    "        file_paths[cls].extend(extra)\n",
    "\n",
    "# Flatten and shuffle\n",
    "all_files = []\n",
    "all_labels = []\n",
    "for i, cls in enumerate(classes):\n",
    "    all_files.extend(file_paths[cls])\n",
    "    all_labels.extend([i] * len(file_paths[cls]))\n",
    "all_files, all_labels = shuffle(all_files, all_labels, random_state=42)\n",
    "\n",
    "# Generator\n",
    "def balanced_generator(file_paths, labels, batch_size):\n",
    "    while True:\n",
    "        for i in range(0, len(file_paths), batch_size):\n",
    "            batch_files = file_paths[i:i+batch_size]\n",
    "            batch_labels = labels[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            for f in batch_files:\n",
    "                img = tf.keras.preprocessing.image.load_img(f, target_size=img_size)\n",
    "                x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "                x = preprocess_input(x)\n",
    "                batch_images.append(x)\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n",
    "\n",
    "train_gen = balanced_generator(all_files, all_labels, batch_size)\n",
    "steps_per_epoch = len(all_files) // batch_size\n",
    "\n",
    "# --------------------------\n",
    "# Validation & test generators\n",
    "# --------------------------\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "val_gen = val_datagen.flow_from_directory(val_dir, target_size=img_size, batch_size=batch_size, class_mode='binary')\n",
    "test_gen = val_datagen.flow_from_directory(test_dir, target_size=img_size, batch_size=batch_size, class_mode='binary', shuffle=False)\n",
    "\n",
    "# --------------------------\n",
    "# DenseNet121 model\n",
    "# --------------------------\n",
    "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=img_size+(3,))\n",
    "base_model.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(1, activation='sigmoid', dtype='float32')  # float32 for mixed precision\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Callbacks\n",
    "# --------------------------\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6, verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='/kaggle/working/fracture_model_best.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# --------------------------\n",
    "# Train Phase 1: Frozen base\n",
    "# --------------------------\n",
    "history1 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=len(val_gen),\n",
    "    epochs=7,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Fine-tune last 50 layers\n",
    "# --------------------------\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(1e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=len(val_gen),\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Save final model with timestamp\n",
    "# --------------------------\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "final_model_name = f\"/kaggle/working/fracture_model_final_{timestamp}.keras\"\n",
    "model.save(final_model_name)\n",
    "print(f\"Final model saved as {final_model_name}\")\n",
    "\n",
    "# --------------------------\n",
    "# Evaluate on test set\n",
    "# --------------------------\n",
    "loss, acc = model.evaluate(test_gen)\n",
    "print(f\"\\nFinal Test Accuracy: {acc:.4f}, Test Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T12:49:10.046870Z",
     "iopub.status.busy": "2025-08-24T12:49:10.046604Z",
     "iopub.status.idle": "2025-08-24T12:49:10.057615Z",
     "shell.execute_reply": "2025-08-24T12:49:10.056869Z",
     "shell.execute_reply.started": "2025-08-24T12:49:10.046849Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_dir = '/kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/train'\n",
    "val_dir = '/kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/val'\n",
    "test_dir = '/kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/test'\n",
    "\n",
    "# Check if the directories exist\n",
    "for directory in [train_dir, val_dir, test_dir]:\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory does not exist: {directory}\")\n",
    "    else:\n",
    "        print(f\"Directory exists: {directory}\")\n",
    "        # List subdirectories\n",
    "        subdirs = os.listdir(directory)\n",
    "        print(f\"Subdirectories in {directory}: {subdirs}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T12:52:30.747354Z",
     "iopub.status.busy": "2025-08-24T12:52:30.746795Z",
     "iopub.status.idle": "2025-08-24T13:03:15.932056Z",
     "shell.execute_reply": "2025-08-24T13:03:15.931426Z",
     "shell.execute_reply.started": "2025-08-24T12:52:30.747327Z"
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Fix truncated images\n",
    "# --------------------------\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# --------------------------\n",
    "# Imports\n",
    "# --------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# --------------------------\n",
    "# Mixed precision\n",
    "# --------------------------\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# --------------------------\n",
    "# Paths\n",
    "# --------------------------\n",
    "train_dir = \"/kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/train\"\n",
    "val_dir   = \"/kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/val\"\n",
    "test_dir  = \"/kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/test\"\n",
    "\n",
    "# --------------------------\n",
    "# Parameters\n",
    "# --------------------------\n",
    "classes = [\"fractured\", \"not fractured\"]  # EXACT folder names\n",
    "img_size = (160, 160)\n",
    "batch_size = 32\n",
    "\n",
    "# --------------------------\n",
    "# Prepare true balanced training generator\n",
    "# --------------------------\n",
    "file_paths = {cls: [os.path.join(train_dir, cls, f) for f in os.listdir(os.path.join(train_dir, cls))] for cls in classes}\n",
    "max_count = max(len(file_paths[cls]) for cls in classes)\n",
    "\n",
    "# Oversample minority class\n",
    "for cls in classes:\n",
    "    n = len(file_paths[cls])\n",
    "    if n < max_count:\n",
    "        extra = np.random.choice(file_paths[cls], max_count - n, replace=True)\n",
    "        file_paths[cls].extend(extra)\n",
    "\n",
    "# Flatten and shuffle\n",
    "all_files = []\n",
    "all_labels = []\n",
    "for i, cls in enumerate(classes):\n",
    "    all_files.extend(file_paths[cls])\n",
    "    all_labels.extend([i] * len(file_paths[cls]))\n",
    "all_files, all_labels = shuffle(all_files, all_labels, random_state=42)\n",
    "\n",
    "# Generator\n",
    "def balanced_generator(file_paths, labels, batch_size):\n",
    "    while True:\n",
    "        for i in range(0, len(file_paths), batch_size):\n",
    "            batch_files = file_paths[i:i+batch_size]\n",
    "            batch_labels = labels[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            for f in batch_files:\n",
    "                img = tf.keras.preprocessing.image.load_img(f, target_size=img_size)\n",
    "                x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "                x = preprocess_input(x)\n",
    "                batch_images.append(x)\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n",
    "\n",
    "train_gen = balanced_generator(all_files, all_labels, batch_size)\n",
    "steps_per_epoch = len(all_files) // batch_size\n",
    "\n",
    "# --------------------------\n",
    "# Validation & test generators\n",
    "# --------------------------\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "val_gen = val_datagen.flow_from_directory(val_dir, target_size=img_size, batch_size=batch_size, class_mode='binary')\n",
    "test_gen = val_datagen.flow_from_directory(test_dir, target_size=img_size, batch_size=batch_size, class_mode='binary', shuffle=False)\n",
    "\n",
    "# --------------------------\n",
    "# DenseNet121 model\n",
    "# --------------------------\n",
    "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=img_size+(3,))\n",
    "base_model.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(1, activation='sigmoid', dtype='float32')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Callbacks\n",
    "# --------------------------\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6, verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='/kaggle/working/fracture_model_best.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# --------------------------\n",
    "# Train Phase 1: Frozen base\n",
    "# --------------------------\n",
    "history1 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=len(val_gen),\n",
    "    epochs=7,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Fine-tune last 50 layers\n",
    "# --------------------------\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(1e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=len(val_gen),\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Save final model with timestamp\n",
    "# --------------------------\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "final_model_name = f\"/kaggle/working/fracture_model_final_{timestamp}.keras\"\n",
    "model.save(final_model_name)\n",
    "print(f\"Final model saved as {final_model_name}\")\n",
    "\n",
    "# --------------------------\n",
    "# Evaluate on test set\n",
    "# --------------------------\n",
    "loss, acc = model.evaluate(test_gen)\n",
    "print(f\"\\nFinal Test Accuracy: {acc:.4f}, Test Loss: {loss:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4854718,
     "sourceId": 8201044,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
